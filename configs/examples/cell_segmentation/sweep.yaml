# Example configuration for HoverNet-Cell-Segmentation
# comment and project setup for wandb
sweep:
  method: bayes
  name: "CellViT-SAM-H"
  metric:
    name: "hubmap_metric"
    goal: minimize
  early_terminate:          # Just applies for bayes method (https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#early_terminate)
    type: hyperband         # Currently: just hyperband
    #...                     # Other options
  run_cap: 1                 # Number of trials for the sweep [int]
# Sweep parameters are defined on the according configuration section. Please be carefull to have only on "parameters" key on every level to avoid doubled key conflicts
# Examples are given below
# Documentation on parameters: https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters

logging:
  mode: online                    # "online" or "offline" [str]
  project: hubmap                  # Name of project to use [str]
  notes: kaggle-hubmap                   # Notes about the run, verbose description [str]
  log_comment: kaggle-hubmap             # Comment to add to name the local logging folder [str]
  tags:                     # List of tags, e.g., ["baseline", "run1"] [str]
    - "SAM-H"
  wandb_dir: wandb_dir                # Direcotry to store the wandb file. CAREFUL: Directory must exists [str]
  log_dir: log_dir                 # Direcotry to store all logging related files and outputs [str]
  level: info                   # Level of logging must be either ["critical", "error", "warning", "info", "debug"] [str]
  log_images: true              # If images should be logged to WandB for this run. [bool] [Optional, defaults to False]
  group: null                   # WandB group tag [str] [Optional, defaults to None]

# seeding
random_seed: 19             # Seed for numpy, pytorch etc. [int]

# hardware
gpu: 0                       # Number of GPU to run experiment on [int]

# setting paths and dataset
data:
  dataset: PanNuke                 # Name of dataset, currently supported: PanNuke [str]
  dataset_path: configs/datasets/PanNuke            # Path to dataset, compare ./docs/readmes/pannuke.md for further details [str]
  train_folds: [0]        # List of fold Numbers to use for training [list[int]]
  # val_split:               # Percentage of training set that should be used for validation. Either val_split or val_fold must be provided, not both. [float]
  val_folds: [1]               # List of fold Numbers to use for validation [list[int]]
  test_folds: [2]              # List of fold Numbers to use for final testing [list[int]]
  num_nuclei_classes: 4      # Number of different nuclei classes (including background!, e.g. 5 nuclei classes + background = 6) [int]
  num_tissue_classes: 0
# model options
model:
  backbone: SAM-H                # Backbone Type: Options are: default, ViT256, SAM-B, SAM-L, SAM-H
  pretrained_encoder: sam_vit_h.pth      # Set path to a pretrained encoder [str]
  pretrained: CellViT-SAM-H-x40.pth              # Path to a pretrained model (.pt file) [str, default None]
  embed_dim: 1280               # Embedding dimension for ViT - typical values are 384 (ViT-S), 768 (ViT-B), 1024 (ViT-L), 1280 (ViT-H) [int]
  input_channels: 3          # Number of input channels, usually 3 for RGB [int, default 3]
  depth: 32                   # Number of Transformer Blocks to use - typical values are 12 (ViT-S), 12 (ViT-B), 24 (ViT-L), 32 (ViT-H) [int]
  num_heads: 16               # Number of attention heads for MHA - typical values are 6 (ViT-S), 12 (ViT-B), 16 (ViT-L), 16 (ViT-H) [int]
  extract_layers: 5          # List of layers to extract for skip connections - starting from 1 with a maximum value equals the depth [int]
  shared_skip_connections: True # If skip connections should be share accross all upsampling branches [bool] [Optional, defaults to False]

# loss function settings (best shown by an example). See all implemented loss functions in base_ml.base_loss module
loss:
  nuclei_binary_map:        # Name of the branch. Possible branches are "nuclei_binary_map", "hv_map", "nuclei_type_map", "tissue_types". If not defined default HoverNet settings are used [str]
    # bce:                    # Name of the loss [str]
    #   loss_fn: xentropy_loss  # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
    #   weight: 1               # Weight parameter [int] [Optional, defaults to 1]
      # args:                   # Parameters for the loss function if necessary. Does not need to be set, can also be empty. Just given as an example here
      #   arg1:                 # 1. Parameter etc. (Name must match to the initialization parameter given for the loss)
    dice:                   # Name of the loss [str]
      loss_fn: dice_loss    # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
      weight: 1             # Weight parameter [int] [Optional, defaults to 1]
    focaltverskyloss:       # Name of the loss [str]
      loss_fn: FocalTverskyLoss # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
      weight: 1             # Weight parameter [int] [Optional, defaults to 1]
      # optional parameters, not implemented yet
  # hv_map:                   # another branch
  #   mse:                   # Name of the loss [str]
  #     loss_fn: mse_loss_maps    # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
  #     weight: 2.5             # Weight parameter [int] [Optional, defaults to 1]
  #   msge:       # Name of the loss [str]
  #     loss_fn: msge_loss_maps # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
  #     weight: 8
  nuclei_type_map:          # another branch
    dice:                   # Name of the loss [str]
      loss_fn: dice_loss    # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
      weight: 0.2             # Weight parameter [int] [Optional, defaults to 1]
    focaltverskyloss:       # Name of the loss [str]
      loss_fn: FocalTverskyLoss # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
      weight: 0.5
      args:
        num_classes: 4
    bce:                    # Name of the loss [str]
      loss_fn: xentropy_loss  # Loss_fn, name is defined in LOSS_DICT in base_ml.base_loss module [str]
      weight: 0.5 
  # tissue_types:             another branch


# training options
training:
  batch_size: 2            # Training Batch size [int]
  epochs: 100               # Number of Training Epochs to use [int]
  unfreeze_epoch: 10          # Epoch Number to unfreeze backbone [int]
  parameters:
    attn_drop_rate:
      min: 0.0
      max: 0.15
    drop_rate:
      values: [0, 0.15, 0.25]          # Dropout rate in attention layer [float] [Optional, defaults to 0]
  drop_path_rate:  0         # Dropout rate in paths [float] [Optional, defaults to 0]
  optimizer: AdamW               # Pytorch Optimizer Name. All pytorch optimizers (v1.13) are supported. [str]
  optimizer_hyperparameter: # Hyperparamaters for the optimizers, must be named exactly as in the pytorch documation given for the selected optimizer
    parameters:
      lr:
        min: 0.00001
        max: 0.001
      betas:
        values:
          [[0.85, 0.85]]
          # [[0.85, 0.9], [0.9, 0.999], [0.85, 0.95]]
      weight_decay:
        min: 0.00001
        max: 0.001      # e.g. betas for Adam
  early_stopping_patience: 5 # Number of epochs before applying early stopping after metric has not been improved. Metric used is total loss. [int]
  scheduler:                # Learning rate scheduler. If no scheduler is selected here, then the learning rate stays constant
    scheduler_type: exponential        # Name of learning rate scheduler. Currently implemented: "constant", "exponential", "cosine". [str]
    gamma: 0.85 # hyperparameters       # gamma [default 0.95] for "exponential", "eta_min" [default 1e-5] for CosineAnnealingLR
  sampling_strategy: random       # Sampling strategy. Implemented are "random", "cell", "tissue" and "cell+tissue" [str] [Optional, defaults to "random"]
  sampling_gamma: 0          # Gamma for balancing sampling. Must be between 0 (equal weights) and 1 (100% oversampling) [float] [Optional, defaults to 1]
  mixed_precision: true

# transformations, here all options are given. Remove transformations by removing them from this section
transformations:
  randomrotate90:           # RandomRotation90
    p: 0.5                     # Probability [float, between 0 and 1]
  horizontalflip:           # HorizontalFlip
    p: 0.5                     # Probability [float, between 0 and 1]
  verticalflip:             # VerticalFlip
    p: 0.5                     # Probability [float, between 0 and 1]
  downscale:                # Downscaling
    p: 0.15                     # Probability [float, between 0 and 1]
    scale: 0.5                 # Scaling factor, maximum should be 0.5. Must be smaller than 1 [float, between 0 and 1]
  blur:                     # Blur
    p: 0.2                     # Probability [float, between 0 and 1]
    blur_limit: 10            # Bluring limit, maximum should be 10, recommended 10 [float]
  gaussnoise:               # GaussianNoise
    p: 0.25                     # Probability [float, between 0 and 1]
    var_limit: 10             # Variance limit, maxmimum should be 50, recommended 10 [float]
  colorjitter:              # ColorJitter
    p: 0.2                     # Probability [float, between 0 and 1]
    scale_setting: 0.25         # Scaling for contrast and brightness, recommended 0.25 [float]
    scale_color: 0.1           # Scaling for hue and saturation, recommended 0.1 [float]
  superpixels:              # SuperPixels
    p: 0.1                     # Probability [float, between 0 and 1]
  zoomblur:                 # ZoomBlur
    p: 0.1                     # Probability [float, between 0 and 1]
  randomsizedcrop:          # RandomResizeCrop
    p: 0.1                     # Probability [float, between 0 and 1]
  elastictransform:         # ElasticTransform
    p: 0.2                     # Probability [float, between 0 and 1]
  normalize:                # Normalization
    mean: [0.5, 0.5, 0.5]                  # Mean for Normalizing, default to (0.5, 0.5, 0.5) [list[float], between 0 and 1 for each entry]
    std: [0.5, 0.5, 0.5]                   # STD for Normalizing, default to (0.5, 0.5, 0.5) [list[float], between 0 and 1 for each entry]
